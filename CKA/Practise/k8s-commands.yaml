CKA

tripura.test1@gmail.com
7205609876


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
mock-1
export KUBECONFIG=/etc/kubernetes/admin.conf
kubectl top nodes -- for checking CPU and nodes memory status
kubectl logs logger-complete-cka04-arch -- for checking the pods


ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
     --name=master \
     --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
     --data-dir /root/default.etcd \
     --initial-cluster=master=https://127.0.0.1:2380 \
     --initial-cluster-token etcd-cluster-1 \
     --initial-advertise-peer-urls=https://127.0.0.1:2380 \
     snapshot restore /opt/cluster1_backup_to_restore.db


     kubectl edit clusterrole blue-role-cka21-arch --context cluster1clusterrole.rbac.authorization.k8s.io/blue-role-cka21-arch edited

     kubectl auth can-i get pods --as=system:serviceaccount:default:blue-sa-cka21-arch

     kubectl --context cluster1 get pod -n kube-system kube-apiserver-cluster1-controlplane  -o jsonpath='{.metadata.labels.component}'

kubectl logs logger-cka03-arch --context cluster3 > /root/logger-cka03-arch-all


#restore etcd

 cd /tmp
 export RELEASE=$(curl -s https://api.github.com/repos/etcd-io/etcd/releases/latest | grep tag_name | cut -d '"' -f 4)
 wget https://github.com/etcd-io/etcd/releases/download/${RELEASE}/etcd-${RELEASE}-linux-amd64.tar.gz
 tar xvf etcd-${RELEASE}-linux-amd64.tar.gz ; cd etcd-${RELEASE}-linux-amd64
 mv etcd etcdctl  /usr/local/bin/
 etcdctl snapshot restore --data-dir /root/default.etcd /opt/cluster1_backup_to_restore.db 

kubectl get event --field-selector involvedObject.name=<pod-name>

kubectl logs purple-curl-cka27-trb

kubectl exec -it purple-app-cka27-trb -- bash

kubectl get event --field-selector involvedObject.name=yello-cka20-trb --context=cluster2


kubectl --context=cluster2 taint nodes cluster2-node01 node=cluster2-node01:NoSchedule --overwrite=true

-=-=-
For this question, please set the context to cluster1 by running:


kubectl config use-context cluster1


There is a persistent volume named apple-pv-cka04-str. Create a persistent volume claim named apple-pvc-cka04-str and request a 40Mi of storage from apple-pv-cka04-str PV.
The access mode should be ReadWriteOnce and storage class should be manual.

Set context to cluster1:

Create a yaml template as below:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apple-pvc-cka04-str
spec:
  volumeName: apple-pv-cka04-str
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Mi
Apply the template:
kubectl apply -f <template-file-name>.yaml


=---=-


For this question, please set the context to cluster1 by running:


kubectl config use-context cluster1


A pod definition file is created at /root/peach-pod-cka05-str.yaml on the student-node. Update this manifest file to create a persistent volume claim called peach-pvc-cka05-str to claim a 100Mi of storage from peach-pv-cka05-str PV (this is already created). Use the access mode ReadWriteOnce.


Further add peach-pvc-cka05-str PVC to peach-pod-cka05-str POD and mount the volume at /var/www/html location. Ensure that the pod is running and the PV is bound.

Set context to cluster1

Update /root/peach-pod-cka05-str.yaml template file to create a PVC to utilise the same in POD template.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: peach-pvc-cka05-str
spec:
  volumeName: peach-pv-cka05-str
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
apiVersion: v1
kind: Pod
metadata:
  name: peach-pod-cka05-str
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
      - mountPath: "/var/www/html"
        name: nginx-volume
  volumes:
    - name: nginx-volume
      persistentVolumeClaim:
        claimName: peach-pvc-cka05-str
Apply the template:
kubectl apply -f /root/peach-pod-cka05-str.yaml


-=-=-=-=-=-
#Vaildating Kubernetes cluster

kubectl version

#Pods

basic unit of deployment is Pods 

kubectl top pods 
kubectl top pods --sort-by-cpu
kubectl top pods --sort-by-memory

kubectl top pods -n namespace --sort-by cpu 

kubectl top pods -A (ALL POD RUNNING)

kubectl top pods -A --sort-by memory

kubectl top pods -A --sort-by cpu

To check node on which pod there

kubectl get pods -o wide

kubectl get pods -l component=kube-apiserver -n kube-system

##Replicaset

kind of controller which takes care of pods if dies and help to recreate and schedule on healter nodes asap.

disaster

Replication-controller and Replicaset

set based selector vs equality based selector


scaling of Replication-controller

kubectl scale rs <RC name> --replicas 3


ETCDC 

ETCD backup and restore

ETCDCTL 

##Rbac

Role and Role binding works at Nmaespace level.

aaction resources

# role
kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods --namespace=dev-ns

# rolebinding
kubectl create rolebinding read-pods --role=pod-reader --user=appuser --namespace=dev-ns


kubectl auth can-i get pods -n dev-ns --user=appuser
kubectl auth can-i list pods -n dev-ns --user=appuser

kubectl get pod nginx-pod -n dev-ns --user=appuser
kubectl get pods -n dev-ns --user=appuser


# Cluster-role
kubectl create clusterrole clusterrole-monitoring --verb=get,list,watch --resource=pods

# Cluster-rolebinding
kubectl create clusterrolebinding clusterrole-binding-monitoring --clusterrole=clusterrole-monitoring --user=appmonitor




##Workload and scheduling

Deployment controller 

scaling
rollout and rollback
deployment stategy
going to previous stage 

application upgrades
self healing
sclae up and down 


stategy type

1recreate - shutdown all env- some downtime-lower env
2rolling update- default - slowly rollout version 
3canary - for new deployment preferred-test new env 
4blue-green env same amount of new env to old env 

kubectl get deployment | grep -i strategy


apt remove containerd
apt update; apt install containerd.io
apt autoremove
apt update; apt install containerd.io
rm /etc/containerd/config.toml
systemctl restart containerd



sudo systemctl enable docker
sudo systemctl restart docker
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
#Hold the packages to being upgrade
sudo apt-mark hold kubelet kubeadm kubectl
#sudo kubeadm init --pod-network-cidr 10.0.0.0/16
apt remove containerd
apt update; apt install containerd.io
apt autoremove
apt update; apt install containerd.io
rm /etc/containerd/config.toml
systemctl restart containerd
#sudo kubeadm init --pod-network-cidr 10.0.0.0/16
#history
#kubectl get nodes
#kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
#gcloud compute networks create example-k8s --subnet-mode custom
#gcloud compute networks subnets create k8s-nodes   --network example-k8s   --range 10.240.0.0/24
#kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/tigera-operator.yaml
#kubectl get nodes
#sudo kubeadm init --pod-network-cidr 10.0.0.0/16
sudo kubeadm init
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/custom-resources.yaml -O
kubectl create -f custom-resources.yaml
kubectl get nodes
systemctl daemon-reload
systemctl enable kubelet
systemctl restart kubelet
systemctl status kubelet
kubectl get nodes
kubectl describe nodes master
kubectl get nodes
sudo apt-get install -y kubelet kubeadm kubectl
#Hold the packages to being upgrade
sudo apt-mark hold kubelet kubeadm kubectl
sud
sudo kubeadm init --pod-network-cidr 10.0.0.0/16
#!/bin/bash
sudo apt update -y && sudo apt upgrade -y
sudo apt-get install -y ca-certificates curl gnupg lsb-release
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
vi k8s.sh
sh k8s.sh
kubectl create -f https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml
kubectl taint nodes --all node-role.kubernetes.io/master-
kubeadm init --pod-network-cidr=10.244.0.0/16
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl apply -f https://raw.githubusercontent.com/antrea-io/antrea/main/build/yamls/antrea.yml
kubeadm init
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
kubectl get nodes
kubeadm token create --print-join-command
kubectl get nodes
history
